import os
import re
import json
from pathlib import Path
from bs4 import BeautifulSoup

# 1) Make sure you have `pip install lunr`
from lunr import lunr

EXPORT_DIR = "SparxEA_HTML_Export"
IGNORED_FILES = {
    "index.htm", "blank.htm", "toc.htm",
    "EAID_9BC9448B_98C1_4215_A154_D41E80507030.htm",
    "EAID_808A1CDA_04E7_4d37_B12F_93F3E2023788.htm"
}

JS_FOLDER = os.path.join(EXPORT_DIR, "js")
os.makedirs(JS_FOLDER, exist_ok=True)

INDEX_JS_PATH = os.path.join(JS_FOLDER, "searchIndex.js")
LOGIC_JS_PATH = os.path.join(JS_FOLDER, "searchLogic.js")

LUNR_CDN = "https://unpkg.com/lunr/lunr.js"

def gather_documents():
    docs = []
    for root, dirs, files in os.walk(EXPORT_DIR):
        for fname in files:
            if fname.lower().endswith(".htm") and fname not in IGNORED_FILES:
                fpath = os.path.join(root, fname)
                relpath = os.path.relpath(fpath, EXPORT_DIR).replace("\\", "/")
                with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                soup = BeautifulSoup(content, "html.parser")
                for tag in soup(["script", "style"]):
                    tag.extract()
                text_body = soup.get_text(separator=" ")
                text_body = re.sub(r"\s+", " ", text_body).strip()
                title_tag = soup.find("title")
                if title_tag and title_tag.string:
                    page_title = title_tag.string.strip()
                else:
                    page_title = fname
                docs.append({
                    "id": relpath,
                    "text": text_body,
                    "title": page_title
                })
    return docs

def write_search_index_js(docs):
    # Build the index with python-lunr
    index = lunr(ref='id', fields=('text',), documents=docs)
    serialized_index = index.serialize()

    # Create a docDictionary for showing titles in the results
    doc_dictionary = {}
    for d in docs:
        doc_dictionary[d["id"]] = {"title": d["title"]}

    with open(INDEX_JS_PATH, "w", encoding="utf-8") as f:
        f.write("// Auto-generated by main.py\n\n")

        # docDictionary for displaying result titles
        f.write("var docDictionary = ")
        f.write(json.dumps(doc_dictionary, ensure_ascii=False))
        f.write(";\n\n")

        # Pre-built index
        f.write("var prebuiltIndex = ")
        f.write(json.dumps(serialized_index, ensure_ascii=False))
        f.write(";\n\n")

        # Load the pre-built index (no on-page building)
        f.write("window.idx = lunr.Index.load(prebuiltIndex);\n")

def write_search_logic_js():
    BASE_URL = "/SparxEA_HTML_Export/"  # or wherever your server serves these files

    with open(LOGIC_JS_PATH, "w", encoding="utf-8") as f:
        # Define doLunrSearch
        f.write("function doLunrSearch(query) {\n")
        f.write("  if (!window.idx) return [];\n")
        f.write("  // Split on whitespace characters (no \\s used).\n")
        f.write("  const tokens = query.trim().split(/[ \\t\\r\\n]+/);\n")
        f.write("  // Append '*' to each token for partial matches.\n")
        f.write("  const wildcardQuery = tokens.map(token => token + '*').join(' ');\n")
        f.write("  // Perform the search in Lunr\n")
        f.write("  return window.idx.search(wildcardQuery);\n")
        f.write("}\n\n")

        # Define handleSearchInput
        f.write("function handleSearchInput(el) {\n")
        f.write("  const query = el.value.trim();\n")
        f.write("  if (!query) {\n")
        f.write("    document.getElementById('lunrSearchResults').innerHTML = '';\n")
        f.write("    return;\n")
        f.write("  }\n")
        f.write("  const results = doLunrSearch(query);\n")
        f.write("  let output = '';\n")
        f.write("  results.forEach(r => {\n")
        f.write("    const docId = r.ref;\n")
        f.write("    const docMeta = window.docDictionary[docId];\n")
        f.write("    const docTitle = docMeta ? docMeta.title : docId;\n")
        f.write(f"    const link = '{BASE_URL}' + docId;\n")
        f.write("    output += `<div><a href=\"${link}\" target=\"_self\">${docTitle}</a></div>`;\n")
        f.write("  });\n")
        f.write("  document.getElementById('lunrSearchResults').innerHTML = output;\n")
        f.write("}\n")

def inject_search_bar():
    import re

    head_pattern = re.compile(r"</head>", flags=re.IGNORECASE)
    body_pattern = re.compile(r"<body([^>]*)>", flags=re.IGNORECASE)

    for root, dirs, files in os.walk(EXPORT_DIR):
        for fname in files:
            if fname.lower().endswith(".htm"):
                fpath = os.path.join(root, fname)

                with open(fpath, "r", encoding="utf-8", errors="ignore") as fr:
                    html = fr.read()

                # If the page already has a search bar, skip
                if 'id="lunrSearchBar"' in html:
                    continue

                relpath = os.path.relpath(fpath, EXPORT_DIR)
                rel_parts = relpath.split(os.sep)
                depth = len(rel_parts) - 1
                up_levels = "../" * depth if depth > 0 else "./"

                # Insert script references
                script_inject = f"""
  <script src="{LUNR_CDN}"></script>
  <script src="{up_levels}js/searchIndex.js"></script>
  <script src="{up_levels}js/searchLogic.js"></script>
  </head>
"""
                def replace_head(_match):
                    return script_inject

                if head_pattern.search(html):
                    html = head_pattern.sub(replace_head, html, count=1)
                else:
                    html = script_inject + html

                # Insert currentPageId
                current_page_script = f"""
<script>
  var currentPageId = "{relpath.replace('\\\\','/')}";
</script>
"""
                search_div = f"""
{current_page_script}
<div id="lunrSearchBar" style="margin:10px;">
  <input type="text" oninput="handleSearchInput(this, currentPageId)" placeholder="Search..." />
  <div id="lunrSearchResults" style="margin-top:5px;"></div>
</div>
"""

                def replace_body(m):
                    return f"<body{m.group(1)}>{search_div}"

                if body_pattern.search(html):
                    html = body_pattern.sub(replace_body, html, count=1)
                else:
                    # fallback if no <body>
                    html = search_div + html

                with open(fpath, "w", encoding="utf-8") as fw:
                    fw.write(html)

def main():
    docs = gather_documents()
    write_search_index_js(docs)
    write_search_logic_js()
    inject_search_bar()

if __name__ == "__main__":
    main()