import os
import re
import json
import argparse
from pathlib import Path
from bs4 import BeautifulSoup


IGNORED_FILES = {"index.htm", "blank.htm", "toc.htm", "EAID_9BC9448B_98C1_4215_A154_D41E80507030.htm", "EAID_808A1CDA_04E7_4d37_B12F_93F3E2023788.htm"}

argparser = argparse.ArgumentParser(description="Injects a search bar into all HTML files in the SparxEA_HTML_Export directory.")
argparser.add_argument("-d", "--data-dir", default="data", help="The directory containing the HTML files.")
args = argparser.parse_args()
DATA_DIR = args.data_dir
print("Export directory:", DATA_DIR)

JS_FOLDER = os.path.join(DATA_DIR, "js")
os.makedirs(JS_FOLDER, exist_ok=True)

INDEX_JS_PATH = os.path.join(JS_FOLDER, "searchIndex.js")
LOGIC_JS_PATH = os.path.join(JS_FOLDER, "searchLogic.js")

LUNR_CDN = "https://unpkg.com/lunr/lunr.js"



def gather_documents():
    docs = []
    for root, dirs, files in os.walk(DATA_DIR):
        for fname in files:
            if fname.lower().endswith(".htm") and fname not in IGNORED_FILES:
                fpath   = os.path.join(root, fname)
                relpath = os.path.relpath(fpath, DATA_DIR).replace("\\", "/")
                with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                soup = BeautifulSoup(content, "html.parser")
                for tag in soup(["script", "style"]):
                    tag.extract()
                text_body = soup.get_text(separator=" ")
                text_body = re.sub(r"\s+", " ", text_body).strip()
                title_tag = soup.find("title")
                if title_tag and title_tag.string:
                    page_title = title_tag.string.strip()
                else:
                    page_title = fname
                docs.append({
                    "id": relpath,
                    "text": text_body,
                    "title": page_title
                })
    return docs

def write_search_index_js(docs):
    with open(INDEX_JS_PATH, "w", encoding="utf-8") as f:
        f.write("// Auto-generated by main.py\n")
        f.write("var documents = ")
        f.write(json.dumps(docs, ensure_ascii=False))
        f.write(";\n\n")
        f.write("var docDictionary = {};\n")
        f.write("documents.forEach(function(d){\n")
        f.write("  docDictionary[d.id] = { title: d.title };\n")
        f.write("});\n\n")
        f.write("""(function(){
            window.idx = lunr(function () {
                this.ref('id');
                this.field('text');
                documents.forEach(doc => this.add(doc), this);
            });
            })();
        """)

def write_search_logic_js():
    BASE_URL = "/SparxEA_HTML_Export/"  # or wherever your server serves these files

    with open(LOGIC_JS_PATH, "w", encoding="utf-8") as f:
        f.write(f"""function doLunrSearch(query) {{
  if (!window.idx) return [];
  return window.idx.search(query);
}}

function handleSearchInput(el) {{
  const query = el.value.trim();
  if (!query) {{
    document.getElementById('lunrSearchResults').innerHTML = "";
    return;
  }}
  const results = doLunrSearch(query);
  let output = "";
  results.forEach(r => {{
    const docId = r.ref;  // e.g. "EARoot/EA19/EA254.htm"
    const docMeta = window.docDictionary[docId];
    const docTitle = docMeta ? docMeta.title : docId;
    // Build a link from the server root
    const link = "{BASE_URL}" + docId;
    output += `<div><a href="${{link}}" target="_self">${{docTitle}}</a></div>`;
  }});
  document.getElementById('lunrSearchResults').innerHTML = output;
}}
""")

def inject_search_bar():
    """
    Injects the search bar + scripts into all *.htm, skipping duplicates.
    """
    import re

    head_pattern = re.compile(r"</head>", flags=re.IGNORECASE)
    body_pattern = re.compile(r"<body([^>]*)>", flags=re.IGNORECASE)

    for root, dirs, files in os.walk(DATA_DIR):
        for fname in files:
            if fname.lower().endswith(".htm"):
                fpath = os.path.join(root, fname)

                with open(fpath, "r", encoding="utf-8", errors="ignore") as fr:
                    html = fr.read()

                # If the page already has a search bar, skip
                if 'id="lunrSearchBar"' in html:
                    continue

                relpath = os.path.relpath(fpath, DATA_DIR)
                rel_parts = relpath.split(os.sep)
                depth = len(rel_parts) - 1
                up_levels = "../" * depth if depth > 0 else "./"

                # Insert script references
                script_inject = f"""
  <script src="{LUNR_CDN}"></script>
  <script src="{up_levels}js/searchIndex.js"></script>
  <script src="{up_levels}js/searchLogic.js"></script>
  </head>
"""
                def replace_head(_match):
                    return script_inject

                if head_pattern.search(html):
                    html = head_pattern.sub(replace_head, html, count=1)
                else:
                    # fallback if no </head>
                    html = script_inject + html

                # Insert currentPageId
                current_page_script = f"""
<script>
  var currentPageId = "{relpath.replace('\\\\','/')}";
</script>
"""
                search_div = f"""
{current_page_script}
<div id="lunrSearchBar" style="margin:10px;">
  <input type="text" oninput="handleSearchInput(this, currentPageId)" placeholder="Search..." />
  <div id="lunrSearchResults" style="margin-top:5px;"></div>
</div>
"""

                def replace_body(m):
                    return f"<body{m.group(1)}>{search_div}"

                if body_pattern.search(html):
                    html = body_pattern.sub(replace_body, html, count=1)
                else:
                    # fallback if no <body>
                    html = search_div + html

                with open(fpath, "w", encoding="utf-8") as fw:
                    fw.write(html)

def main():
    docs = gather_documents()
    write_search_index_js(docs)
    write_search_logic_js()
    inject_search_bar()

if __name__ == "__main__":
    main()
